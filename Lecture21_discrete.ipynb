{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = firebrick>Lecture 21: OLS Regression with Discrete Dependent Variables</font><a id='home'></a><br>\n",
    "    \n",
    "We continue to learn about the statsmodels package [(docs)](https://devdocs.io/statsmodels/), which provides functions for formulating and estimating statistical models. In this notebook we take on models in which the dependent variable is discrete. In the examples below, the dependent variable is binary (which makes it easier to visualize). At the end of the lecture, we extend the analysis to dependent variables with many discrete values.  \n",
    "\n",
    "[Here](http://www.statsmodels.org/0.6.1/examples/notebooks/generated/discrete_choice_overview.html) is a nice overview of the discrete choice models in statsmodels. \n",
    "\n",
    "The agenda for today's lecture is as follows:\n",
    "\n",
    "1. [Math Primer](#math)\n",
    "\n",
    "\n",
    "2. [Probit Regression](#probit)\n",
    "\n",
    "\n",
    "3. [Logit Regression](#logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=orange>Class Announcements</font> \n",
    "\n",
    "None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Math Primer ([top](#home))<a id=\"math\"></a>\n",
    "\n",
    "So far we've been dealing with continuous dependant (ie, LHS) variables such as hours worked. A lot of outcomes we observe and are interested in are not continuous, however. For example, labor force participation in the United States is roughly 65% so the choice of whether to work or not appears to be a significant one. \n",
    "\n",
    "Suppose our dependant variable Y is binary (ie, zero or one). For example, Y may represent presence/absence of a certain condition, success/failure of some device, answer yes/no on a survey, etc. We also have a vector of regressors X which we think influence Y. As before, suppose we also have an error term $\\epsilon$ which is distributed from some distribution. Define $Y^*$ as some latent (ie, we can't actually observe) variable where \n",
    "\n",
    "$$\n",
    "Y^*= X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "and we think $Y=1$ whenever $Y^*>0$, or equivalently whenever $X\\beta + \\epsilon>0$. Define $P(Y=1|X)$ as the 'probability Y is equal to one conditional on the variables X.' It follows then that\n",
    "\n",
    "$$\n",
    "P(Y=1|X)= P(Y^*>0)\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow P(\\epsilon < X\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Probit Regression ([top](#home))<a id=\"probit\"></a>\n",
    "\n",
    "Note that $P(\\epsilon < X\\beta)$ is the definition of a CDF. Suppose we specify that $\\epsilon$ is drawn iid from a standard Normal distribution. With this added assumption, we can do a lot more:\n",
    "\n",
    "$$\n",
    "P(Y=1|X)= \\Phi(X\\beta)\n",
    "$$\n",
    "\n",
    "where $\\Phi()$ is the CDF for the standard Normal distribution. The *likelihood* we observe a single observation ($Y_j=1$ or $Y_j=0$) is therefore\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta;y_j,x_j)= \\Phi(x_j\\beta)^{y_j}\\times(1-\\Phi(x_j\\beta))^{1-y_j}.\n",
    "$$\n",
    "\n",
    "The first part ($\\Phi(X\\beta)^{y_j}$) gets turned on when $y_j=1$ while the second part gets turned on when $y_j=0$. We can therefore solve for the $\\beta$ vector to best match the data by maximizing the 'likelihood' function; ie, \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta;Y,X)= \\Pi_{j=1}^J\\Phi(x_j\\beta)^{y_j}\\times(1-\\Phi(x_j\\beta))^{1-y_j}\n",
    "$$\n",
    "\n",
    "This looks complicated but it's really just a simple maxmization problem like OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example: Gambling\n",
    "\n",
    "When we're talking probability, there is no better example than gambling. Actually, gambling is the source (inspiration?) for a lot of the probability theory we have today. Since relativity and quantum mechanics use probability heavilly, let's attribute that to gambling too.\n",
    "\n",
    "The file 'pntsprd.dta' contains data about vegas betting. The complete variable list is [here](http://fmwww.bc.edu/ec-p/data/wooldridge/pntsprd.des). We will use `favwin` which is equal to 1 if the favored team won and zero otherwise and `spread` which holds the betting spread. In this context, a spread is the number of points that the favored team must beat the unfavored team by in order to be counted as a win by the favored team.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                    # for data handling\n",
    "import numpy as np                     # for numerical methods and data structures\n",
    "import matplotlib.pyplot as plt        # for plotting\n",
    "import seaborn as sea                  # advanced plotting\n",
    "\n",
    "import statsmodels.formula.api as smf  # provides a way to directly spec models from formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas read_stata method to get the stata formatted data file into a DataFrame.\n",
    "vegas = pd.read_stata('./Data/pntsprd.dta')\n",
    "\n",
    "# Take a look...so clean!\n",
    "vegas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.scatter( vegas['spread'], vegas['favwin'], facecolors='none', edgecolors='red')\n",
    "\n",
    "ax.set_ylabel('favored team outcome (win = 1, loss = 0)')\n",
    "ax.set_xlabel('point spread')\n",
    "ax.set_title('The data from the point spread dataset')\n",
    "\n",
    "sea.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "\n",
    "We begin with the linear probability model. The model is \n",
    "\n",
    "$$\\text{Pr}(favwin=1 \\mid spread) = \\beta_0 + \\beta_1 spread + \\epsilon .$$\n",
    "\n",
    "There is nothing new here technique-wise. Let's start with OLS which is like pretending the Y variable is continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodels adds a constant for us...\n",
    "res_ols = smf.ols('favwin ~ spread', data=vegas).fit()\n",
    "\n",
    "print(res_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing with t-test\n",
    "If bookies were all-knowing, the spread would **exactly** account for the predictable winning probability and all we would be left with is the noise --- the intercept should be one-half. Is it true in the data? We can use the `t_test( )` method of the results object to perform t-tests. \n",
    "\n",
    "The null hypothesis is $H_0: \\beta_0 = 0.5$ and the alternative hypothesis is $H_1: \\beta_0 \\neq 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test = res_ols.t_test('Intercept = 0.5')\n",
    "print(t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear probability models have some problems. Perhaps the biggest one is that there is no guarantee that the predicted probability lies between zero and one! \n",
    "\n",
    "We can use the `predictedvalues` attribute of the results object to recover the fitted values of the y variables. Let's plot them and take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.scatter(vegas['spread'], res_ols.fittedvalues,  facecolors='none', edgecolors='red')\n",
    "ax.axhline(y=1.0, color='grey', linestyle='--')\n",
    "\n",
    "ax.set_ylabel('pedict probability of winning')\n",
    "ax.set_xlabel('point spread')\n",
    "ax.set_title('Predicted winning probabilities from an OLS model')\n",
    "\n",
    "sea.despine(ax=ax, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's account for the discreteness and estimate with probit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_probit = smf.probit('favwin ~ spread', data=vegas).fit()\n",
    "print(res_probit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the top: \"Optimization terminated successfully...\" That's because with probit there is no analytical solution like there is with OLS. Instead, the computer has to maximize the likelihood function by taking a guess for an initial $\\beta$ and then iterating using calculus to make smart choices.\n",
    "\n",
    "The coefficients are very different. Just look at the intercept! That's in large part b/c the coefficients have a different meaning in a probabilistic model. In order to determine the effect on Y, we have to run the coefficient through the distributional assumption, here Normal. When we do this, we call the results 'marginal effects.' The math is pretty straight-forward -- but then again recovering marginal effects is standard stuff so there's a method for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margeff = res_probit.get_margeff('mean')\n",
    "print(margeff.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so a unit increase in the spread is correlated with a (statistically significant) 2.5% increase in the probability the team wins. Makes sense -- otherwise those bright, shiny Vegas lights wouldn't be so shiny.\n",
    "\n",
    "Note that the marginal effect calculation required us to take a stand on from where we calculated the derivative. In a linear model like OLS, the derivative is just the coefficients and those are constant. Here, the model is non-linear (b/c of the Normal distribution) so the derivative changes depending on where we choose. The average is the standard though skewed data might make the median more resonable.\n",
    "\n",
    "Let's take a look at the marginal effects at different points in the data. Note that the reported marginal effect above is located at the intersection of the marginal effects plot and the vertical dashed line indicating the average spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm # import functions related to the normal distribution\n",
    "\n",
    "y = norm.pdf(res_probit.fittedvalues,0,1)*res_probit.params.spread\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "avg_spread = np.mean(vegas['spread'])\n",
    "\n",
    "# Create the marginal effects\n",
    "ax.scatter(vegas['spread'],y, color='black', label = 'marg. effects')\n",
    "\n",
    "ax.set_ylabel('estimated marginal effect')\n",
    "ax.set_xlabel('point spread')\n",
    "ax.set_title('plotting marginal effects')\n",
    "\n",
    "ax.legend(frameon=False,loc='upper right', bbox_to_anchor=(0.9, 0.7), fontsize=14)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.axvline(x=avg_spread, color='red', linestyle='--')\n",
    "ax.text(avg_spread+.5,0.035,'Average Spread',fontsize=14)\n",
    "ax.set_ylim([-1e-3,0.04])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the predicted values. In OLS this was easy. Here, things are (for some bizarre reason) more complicated -- we have to run the $X\\hat\\beta$ interactions through the standard Normal distribution ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probit = norm.cdf(res_probit.fittedvalues,0,1)  # Standard Normal (ie, mean = 0, stdev = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the estimated probabilty of the favored team winning and the actual data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.scatter(vegas['spread'], pred_probit,  facecolors='none', edgecolors='red', label='predicted')\n",
    "ax.scatter(vegas['spread'], vegas['favwin'],  facecolors='none', edgecolors='blue', label = 'data')\n",
    "ax.axhline(y=1.0, color='grey', linestyle='--')\n",
    "\n",
    "# Create the line of best fit to plot\n",
    "p = res_ols.params                            # params from the OLS model linear probability model\n",
    "x = range(0,35)                               # some x data\n",
    "y = [p.Intercept + p.spread*i for i in x]     # apply the coefficients \n",
    "ax.plot(x,y, color='black', label = 'linear prob.')\n",
    "\n",
    "ax.set_ylabel('pedict probability of winning')\n",
    "ax.set_xlabel('point spread')\n",
    "ax.set_title('Predicted winning probabilities from a probit model')\n",
    "\n",
    "ax.legend(frameon=False,loc='upper right', bbox_to_anchor=(0.9, 0.7), fontsize=14)\n",
    "sea.despine(ax=ax, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression (aka Logit) ([top](#home))<a id=\"logit\"></a>\n",
    "\n",
    "Our framework is actually pretty flexible so we can use different distributions. The other popular distributional assumption is to assume the $\\epsilon$ errors come from a Logistic distribution. Why Logistic? Because the result is a nice simple function for the probability:\n",
    "\n",
    "$$\\text{P} = \\frac{\\exp \\left({\\beta_0+\\beta_1 spread}\\right)}{1+\\exp \\left({\\beta_0+\\beta_1 spread}\\right)},$$\n",
    "\n",
    "and we predict a team wins when ever $\\text{prob} \\ge 0.5$. As above, the computer chooses $\\beta$ to best fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-8, 8, 0.1);\n",
    "    \n",
    "# Determine Y\n",
    "Y = 1/(1+np.exp(-X))\n",
    "\n",
    "# Create Figure\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "ax.axhline(y=0.5, color='red',linewidth=1,ls='--')\n",
    "\n",
    "ax.annotate('Class One: Y=1',xy=(-6,.6),va='center',ha='left',size=18)\n",
    "ax.annotate('Class Two: Y=0',xy=(-6,.4),va='center',ha='left',size=18)\n",
    "ax.axhspan(.5, 1, alpha=0.2, color='blue')\n",
    "\n",
    "ax.plot(X,Y, color = 'black')\n",
    "\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_yticks(np.arange(0, 1.01, step=0.1))\n",
    "ax.set_xlim(-8,8)\n",
    "ax.set_xlabel('Independent Variable (X)',size=14)\n",
    "ax.set_ylabel('Dependent Variable (Y)',size=14)\n",
    "ax.set_title('Logistic Function and Decision Rule',size=20)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the logit model with `logit( )` method from `smf` in a way similar to `probit`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_logit = smf.logit('favwin ~ spread', data=vegas).fit()\n",
    "print(res_logit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, interpreting logit coefficients is bit more complicated. The probability that a team wins is given by the expression\n",
    "\n",
    "$$\\text{prob} = \\frac{\\exp \\left({\\beta_0+\\beta_1 spread}\\right)}{1+\\exp \\left({\\beta_0+\\beta_1 spread}\\right)}$$\n",
    "\n",
    "Our marginal effects will hammer $X\\hat\\beta$ through the above non-linear function to derive the marginal effects. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margeff = res_logit.get_margeff('mean')\n",
    "print(margeff.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again plot the estimated probabilty of the favored team winning and the actual data but now let's compare the implications of our distributional assumptions. First, generate predicted values using `numpy` and the above expression for the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logit = np.exp(res_logit.fittedvalues) /( 1+np.exp(res_logit.fittedvalues) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot probit vs logit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.scatter(vegas['spread'], pred_logit,  facecolors='none', edgecolors='red', label='predicted-logit')\n",
    "ax.scatter(vegas['spread'], pred_probit,  facecolors='none', edgecolors='black', label='predicted-probit')\n",
    "ax.scatter(vegas['spread'], vegas['favwin'],  facecolors='none', edgecolors='blue', label = 'data')\n",
    "ax.axhline(y=1.0, color='grey', linestyle='--')\n",
    "\n",
    "# Create the line of best fit to plot\n",
    "p = res_ols.params                            # params from the OLS model linear probability model\n",
    "x = range(0,35)                               # some x data\n",
    "y = [p.Intercept + p.spread*i for i in x]     # apply the coefficients \n",
    "ax.plot(x,y, color='black', label = 'linear prob.')\n",
    "\n",
    "ax.set_ylabel('pedict probability of winning')\n",
    "ax.set_xlabel('point spread')\n",
    "ax.set_title('Predicted winning probabilities from logit and probit models')\n",
    "\n",
    "ax.legend(frameon=False,loc='upper right', bbox_to_anchor=(0.9, 0.7), fontsize=14)\n",
    "sea.despine(ax=ax, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the probit and logit models are nearly on top of eachother. That's a common occurrence. In practice, the models are often interchangeable and the practitioner will choose one over the other because in their setting one may have some slightly better properties (e.g., more intuitive intrepretation of the marginal effects). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Practice</font>\n",
    "\n",
    "1. Load the data 'apple.dta'. The data dictionary can be found [here](http://fmwww.bc.edu/ec-p/data/wooldridge/apple.des). The variable `ecolbs` is purchases of eco-friendly apples (whatever that means).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a variable named `ecobuy` that is equal to 1 if the observation has a positive purchase of eco-apples (i.e., ecolbs>0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Estimate a linear probability model relating the probability of purchasing eco-apples to household characteristics. \n",
    "\n",
    "$$\\text{ecobuy} = \\beta_0 + \\beta_1 \\text{ecoprc} + \\beta_2 \\text{regprc} + \\beta_3 \\text{faminc} + \\beta_4 \\text{hhsize} + \\beta_5 \\text{educ} + \\beta_6 \\text{age} +  \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many estimated probabilities are negative? Are greater than one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now estimate the model as a probit; i.e., <br><br>\n",
    "$$\\text{Pr}(\\text{ecobuy}=1 \\mid X) = \\Phi \\left(\\beta_0 + \\beta_1 \\text{ecoprc} + \\beta_2 \\text{regprc} + \\beta_3 \\text{faminc} + \\beta_4 \\text{hhsize} + \\beta_5 \\text{educ} + \\beta_6 \\text{age} \\right),$$<br>where $\\Phi( )$ is the CDF of the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Compute the **marginal effects** of the coefficients at **the means** and print them out using `summary()`. Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Re-estimate the model as a logit model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Compute the marginal effects of the logit coefficients at the averages in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. We haven't done much data wrangling lately. I'm feeling a bit sad; I miss shaping data. Create a pandas DataFrame with the row index  'ecoprc', 'regprc', 'faminc', 'hhsize', 'educ', and 'age'. The columns should be labeled 'logit', 'probit', and 'ols'. The columns should contain the marginal effects for the logit and probit models and the coefficients from the ols model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
